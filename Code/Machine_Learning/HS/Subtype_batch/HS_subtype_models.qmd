---
title: "HS Subtype models"
author: "ThomThom"
format: html
editor: source
---

# Preparatory steps

Loading packages Store package names in a vectors for ease of access and to load them easily

```{r Setup, echo=FALSE, warning=FALSE}

# Define the vector of package names needed for the project
PACKAGES <- c(
  "xgboost",         # Extreme Gradient Boosting algorithm for classification/regression
  "ggplot2",         # Data visualization package based on the grammar of graphics
  "RColorBrewer",    # Color palettes for enhanced data visualization
  "tidymodels",      # Tidy modeling framework for machine learning workflows
  "tidyverse",       # Collection of packages for data manipulation, analysis, and visualization
  "finetune",        # Tools for model tuning and optimization
  "themis",          # Techniques for handling class imbalance in machine learning
  "gprofiler2",      # Gene enrichment and pathway analysis for gene lists
  "future",          # Enables parallel processing for faster computations
  "data.table",      # High-performance data manipulation package
  "gt",              # For creating display tables
  #"gtsummary",       # For summary tables with statistics and formatted output
  "butcher",         # Reduces memory load of models by removing unnecessary objects
  "UBL",              # Utility-based learning techniques for imbalanced data
  "ranger",
  "kernelshap",
  "shapviz"
)

# Use purrr::walk to load all packages
purrr::walk(PACKAGES, library, character.only = TRUE)
gc()

plan(multisession, workers = 10)
nbrOfWorkers()

RANDOM_SEED <- 1234
#plan("multisession", workers = 15,)
#plan(multisession, workers = 15)
#plan(sequential)


```

## Managing the data to use

```{r Data to use}
#| warning: false
# Load the data
# Read the CPM table, set the first column as row names, and transpose the data
ML_data <- fread("../../ALLDEGs/CPM_log_HS_tumorvscontrol_70.csv") 
# Convert Ensembl IDs to gene names
gene_names <- gconvert(query = ML_data$V1, organism = "hsapiens", 
         target = "ENSG", mthreshold = Inf, filter_na = TRUE)
#ML_data$V1 == gene_names$target

ML_data$V1 <- gene_names$name

#ML_data$V1 <- gene_names$name
ML_data <- ML_data %>%
  tibble() %>%
  column_to_rownames(var = "V1") %>%  # Set first column as row names
  t() %>%  # Transpose the data for easier handling
  as_tibble(rownames = "sample")  # Convert to tibble and keep row names as a column

# Load validation data
ML_validation <- fread("../../ALLDEGs/cpm_table_log_30.csv") 
# Convert Ensembl IDs to gene names
gene_names <- gconvert(query = ML_validation$V1, organism = "hsapiens", 
         target = "ENSG", mthreshold = Inf, filter_na = TRUE)

ML_validation$V1 <- gene_names$name

ML_validation <- ML_validation %>%
  tibble() %>%
  distinct(V1, .keep_all = TRUE) %>%
  column_to_rownames(var = "V1") %>%  # Set first column as row names
  t() %>%  # Transpose the data for easier handling
  as_tibble(rownames = "sample")  # Convert to tibble and keep row names as a column

# Load metadata for sample subtypes
# Read the metadata file and rename the sample column for consistency
samples_70_explained <- read_csv("../../ALLDEGs/info_samples_70.csv") %>%
  tibble() %>%
  select(-...1) 

samples_30_explained <- read_csv("../../ALLDEGs/info_samples_30.csv") |>
  tibble() |>
  select(-...1)

# Merge and filter the data 70
ML_data <- ML_data %>%
  left_join(samples_70_explained, by = "sample") %>%  # Join tumor/control info
  filter(condition != "H") %>%  # Filter out samples with condition 'H'
  select(-c(type, age, replicate, condition))  # Remove unnecessary columns

# Convert to data frame and set sample as row names
ML_data <- data.frame(ML_data)  # Convert tibble to data frame
rownames(ML_data) <- ML_data$sample  # Set row names to 'sample'
ML_data$sample <- NULL  # Remove the sample column

# Merge and filter the data 30
ML_validation <- ML_validation %>%
  left_join(samples_30_explained, by = "sample") %>%  # Join tumor/control info
  filter(condition != "H") %>%  # Filter out samples with condition 'H'
  select(-c(type, age, replicate, condition))  # Remove unnecessary columns

# Convert to data frame and set sample as row names
ML_validation <- data.frame(ML_validation)  # Convert tibble to data frame
rownames(ML_validation) <- ML_validation$sample  # Set row names to 'sample'
ML_validation$sample <- NULL  # Remove the sample column

# Identify indices of samples with 'Unknown' cell types
unknown_indices <- which(ML_data$Cell_type == 'Unknown')  # Find 'Unknown' cell type indices
table(ML_data$Cell_type)  # Display counts of each cell type

#fake_data <- read_csv("input/fake_data_FM.csv") |>  filter(Cell_type != "PreB")
#colnames(fake_data) <- colnames(ML_data)
#ML_data <- rbind(ML_data, fake_data)

# Clean up unnecessary variables to free memory
#rm(info_samples_Tumor_Control, samples_subtypes_explained)  # Remove temporary metadata


```

## Number coating

```{r Current otimal way  }
# Number encoding of categorical values
# Specify the columns to be label encoded
columns_to_encode <- c("Cell_type")

# Create separate datasets based on 'Cell_type'
# Create a new data frame without rows where 'Cell_type' is "Unknown"
my_data_train <- subset(ML_data, Cell_type != "Unknown")
# Create a separate dataset for rows where 'Cell_type' is "Unknown"
Unknown_data <- subset(ML_data, Cell_type == "Unknown")

# Convert specified columns to factor type
my_data_train <- my_data_train %>%
  mutate_at(columns_to_encode, as.factor)  # Convert 'type' to factor
Unknown_data <- Unknown_data %>% 
  mutate_at(columns_to_encode, as.factor)  # Convert 'type' to factor in Unknown_data

# Get the levels from ML_data
common_levels <- levels(my_data_train$Cell_type)

# Set the levels
ML_validation$Cell_type <- factor(ML_validation$Cell_type, levels = common_levels)

## Optional: Convert factors to numeric (commented out)
# train_data_numeric <- my_data_train %>% mutate_at(columns_to_encode, as.numeric)  # Convert factors to numeric

# Create a dictionary-like structure to store the levels of categorical variables
## The order corresponds to the number (commented out)
# my_levels <- list(Cell_type = levels(my_data_train$Cell_type))  # Store levels of 'Cell_type'

# Clean up unnecessary variables to free memory
rm(columns_to_encode)  
```

## Split tidymodels

```{r Split tidymodels }

set.seed(RANDOM_SEED)

# === Train/Test Split ===
train_data <- my_data_train
test_data  <- ML_validation

cli::cli_h3("\n[INFO] Train ML data before NCL:\n")
print(table(train_data$Cell_type))


Classes <- list("B" = 8, "PreB" = 1, "PreT" = 4, "T" = 2)
# === Advanced Class Balancing using UBL ===
train_data <- SMOGNClassif(Cell_type ~ ., train_data, C.perc =  Classes,k = 5)
#train_data <- SmoteClassif(Cell_type ~ ., train_data, C.perc = list("B" = 8, "PreB" = 1, "PreT" = 4, "T" = 2),k = 5)
cli::cli_h3("\n[INFO] Train ML data after SMOTEGNn:\n")
print(table(train_data$Cell_type))
train_data <- NCLClassif(Cell_type ~ ., train_data,k = 5) #,Cl = c("B", "PreT", "T"),
cli::cli_h3("\n[INFO] Train ML data after NCL:\n")
print(table(train_data$Cell_type))

#train_data <- train_data |> mutate(Weights = hardhat::importance_weights(as.integer(Cell_type)))

# Calculate class frequencies
class_counts <- table(train_data$Cell_type)
total_samples <- sum(class_counts)
num_classes <- length(class_counts)

# Compute weights inversely proportional to class frequencies
class_weights <- (total_samples / (num_classes * class_counts))^0.5
class_weights <- c(as.numeric(class_weights))
class_weights <- class_weights / mean(class_weights)
names(class_weights) <- names(class_counts)

print(round(class_weights, 3))
# Assign weights to each observation
train_data <- train_data %>%
  mutate(Weights = class_weights[as.character(Cell_type)])

train_data$Weights <- hardhat::new_importance_weights(c(train_data$Weights))


# === Preprocessing Recipe ===
recipe <- recipe(Cell_type ~ ., data = train_data) 

# === Recipe Prep and Bake ===
rec_prep   <- prep(recipe, training = train_data)
baked_data <- bake(rec_prep, new_data = NULL)

cli::cli_h3("\n[INFO] Baked (final) ML training data:\n")
print(table(baked_data$Cell_type))

# === Cross-validation Splits ===
v_folds <- vfold_cv(train_data, strata = Cell_type, v = 5, repeats = 3)
#nested_cv(data = train_data)

# === Evaluation Metrics ===
class_metrics <- metric_set(kap, mcc, bal_accuracy, f_meas, roc_auc, ppv)


```

# Naive Bayes
```{r}
set.seed(RANDOM_SEED)

library(discrim)
library(tidymodels)

# Define the Naive Bayes model spec with tunable hyperparameters
nb_spec <- 
  naive_Bayes(
    Laplace = tune(),
    smoothness = tune()
  ) %>%
  set_engine("naivebayes") %>%
  set_mode("classification")

# Define the hyperparameter search space
nb_param_space <- extract_parameter_set_dials(nb_spec) %>%
  update(
    Laplace = Laplace(range = c(0, 5)),
    smoothness = smoothness(range = c(1e-4, 1))
  )

# Generate a Latin hypercube grid
nb_grid <- grid_space_filling(nb_param_space, size = 50, type = "latin_hypercube")

# Build the modeling workflow
nb_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(nb_spec)

# Phase 1: Grid search tuning
nb_grid_results <- tune_grid(
  nb_wf,
  resamples = v_folds,
  grid = nb_grid,
  metrics = class_metrics
)

# Phase 2: Bayesian optimization
nb_bayes_results <- tune_bayes(
  nb_wf,
  resamples = v_folds,
  initial = nb_grid_results,
  param_info = nb_param_space,
  iter = 30,
  metrics = class_metrics,
  control = control_bayes(no_improve = 10, verbose_iter = TRUE)
)

# Review the top-performing configurations by MCC
show_best(nb_bayes_results, metric = "mcc")

# Extract best hyperparameters
nb_best <- select_best(nb_bayes_results, metric = "mcc")

# Finalize workflow with best hyperparameters
final_nb_wf <- finalize_workflow(nb_wf, nb_best)

# Train final model on full training data
nb_final_fit <- fit(final_nb_wf, data = train_data)

# Print model summary
print(nb_final_fit)
```


## Results NB
```{r}
set.seed(RANDOM_SEED)

# Resample fit for stack integration and robust performance estimation
nb_resamples <- fit_resamples(
  final_nb_wf,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

# Collect aggregate resample metrics
collect_metrics(nb_resamples)

# Predict on test set
test_predictions <- nb_final_fit %>%
  predict(new_data = test_data) %>%
  dplyr::mutate(Cell_type = test_data$Cell_type)

# Compute confusion matrix
conf_mat_nb <- conf_mat(test_predictions, truth = Cell_type, estimate = .pred_class)
nb_metrics_summary <- summary(conf_mat_nb)
nb_metrics_summary

# Heatmap visualization of confusion matrix
autoplot(conf_mat_nb, type = "heatmap") +
  labs(title = "Confusion Matrix — Naive Bayes Classification")

# Optionally export metrics
# write_csv(nb_metrics_summary, "naive_bayes_metrics.csv")

```


# AORSF model
```{r AORSF model}

set.seed(RANDOM_SEED)

library(bonsai)
library(C50)
library(tidymodels)

# Define the AORSF model specification with tunable hyperparameters
aorsf_spec <- 
  rand_forest(
    mtry = tune(),
    min_n = tune(),
    trees = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("") %>%
  translate()

# Define the hyperparameter search space
aorsf_param_space <- extract_parameter_set_dials(aorsf_spec) %>%
  finalize(train_data) %>%
  update(
    mtry = mtry(range = c(5L, 89L)),        # Adjust according to your feature count
    min_n = min_n(range = c(3L, 15L)),      # Minimum observations per leaf
    trees = trees(range = c(50, 100L))   # Number of trees
  )

# Generate a Latin hypercube grid
aorsf_grid <- grid_space_filling(aorsf_param_space, size = 50, type = "latin_hypercube")

# Build the modeling workflow
aorsf_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(aorsf_spec) %>%
  add_case_weights(Weights)

# Phase 1: Grid search tuning
aorsf_grid_results <- tune_grid(
  aorsf_wf,
  resamples = v_folds,
  grid = aorsf_grid,
  metrics = class_metrics
)

# Phase 2: Bayesian optimization
aorsf_bayes_results <- tune_bayes(
  aorsf_wf,
  resamples = v_folds,
  initial = aorsf_grid_results,
  param_info = aorsf_param_space,
  iter = 30,
  metrics = class_metrics,
  control = control_bayes(no_improve = 10, verbose_iter = TRUE)
)

# Review the top-performing configurations by MCC
show_best(aorsf_bayes_results, metric = "mcc")

# Extract best hyperparameters
aorsf_best <- select_best(aorsf_bayes_results, metric = "mcc")

# Finalize workflow with best hyperparameters
final_aorsf_wf <- finalize_workflow(aorsf_wf, aorsf_best)

# Train final model on full training data
aorsf_final_fit <- fit(final_aorsf_wf, data = train_data,)

# Print model summary
print(aorsf_final_fit)


```


## Results AORSF model

```{r Plots for AORSF model}

set.seed(RANDOM_SEED)

# Resample fit for stack integration and robust performance estimation
aorsf_resamples <- fit_resamples(
  final_aorsf_wf,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

# Collect aggregate resample metrics
collect_metrics(aorsf_resamples)

# Predict on test set
test_predictions <- aorsf_final_fit %>%
  predict(new_data = test_data) %>%
  dplyr::mutate(Cell_type = test_data$Cell_type)

# Compute confusion matrix
conf_mat_rf <- conf_mat(test_predictions, truth = Cell_type, estimate = .pred_class)
rf_metrics_summary <- summary(conf_mat_rf)
rf_metrics_summary

# Heatmap visualization of confusion matrix
autoplot(conf_mat_rf, type = "heatmap") +
  labs(title = "Confusion Matrix — AORSF Classification")

# Optionally export metrics
# write_csv(rf_metrics_summary, "aorsf_rf_metrics.csv")

vip::vip(object = extract_fit_engine(aorsf_final_fit))

```

# Random Forest:

```{r RF with cross validation from tidymodels}
set.seed(RANDOM_SEED)
# Step 1: Define RF specification with tunable hyperparameters
rf_spec <- 
  rand_forest(
    min_n = tune(),
    trees = tune(),
    mtry = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "permutation") |> #permutation
  translate()

# Step 2: Define full parameter search space
rf_param_space <- extract_parameter_set_dials(rf_spec) %>%
  finalize(train_data) %>%
  update(
    min_n = min_n(range = c(3L, 15L)),
    trees = trees(range = c(500L, 1500L)),
    mtry = mtry(range = c(5L, 89L)) # Adjust to your feature space
  )

# Step 3: Create initial space-filling grid
rf_grid <- grid_space_filling(rf_param_space, size = 50, type = "latin_hypercube")

# Step 4: Build tuning workflow
rf_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_spec) |>
  add_case_weights(col = Weights)

# Step 5: Run tuning (grid → annealing → bayes)
rf_tune_grid <- tune_grid(
  rf_workflow,
  resamples = v_folds,
  grid = rf_grid,
  metrics = class_metrics
)

rf_tune_bayes <- tune_bayes(
  rf_workflow,
  resamples = v_folds,
  initial = rf_tune_grid,
  param_info = rf_param_space,
  iter = 30,
  metrics = class_metrics,
  control = control_bayes(no_improve = 10, verbose_iter = TRUE)
)

# Step 6: Inspect best MCC
show_best(rf_tune_bayes, metric = "mcc")

# Step 7: Select best by MCC or mcc
rf_best_params <- select_best(rf_tune_bayes, metric = "mcc")

# Step 8: Finalize and fit
final_rf_workflow <- finalize_workflow(rf_workflow, rf_best_params)

rf_fit <- fit(final_rf_workflow, data = train_data)

# Step 9: Print model and optionally butcher
print(rf_fit)
```

## Results RF

```{r Plots for RF}
# Set seed for reproducibility
set.seed(RANDOM_SEED)

# Fit the model to resamples (this is what stacks needs)
RF_res <- fit_resamples(
  final_rf_workflow,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

collect_metrics(RF_res)

# Extract predictions from the trained random forest model
predictions <- rf_fit %>%
  predict(new_data = test_data) %>% # Generate predictions on the test data
  dplyr::mutate(Cell_type = test_data$Cell_type) # Add true cell type to predictions

# Create a confusion matrix to evaluate model performance
confusion <- conf_mat(predictions, truth = Cell_type, estimate = .pred_class)
metrics_df <- summary(confusion) # Summarize confusion matrix metrics
metrics_df # Display metrics

# Visualize confusion matrix as a heatmap
autoplot(confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - RF HS Subtype Batch Heatmap")

#write_csv(metrics_df, "RF_tuned_metrics.csv")
```


# K-Nearest Neighbors (KNN) Model:

```{r KKNN with CV}
set.seed(RANDOM_SEED)

# Define KNN model spec with tuneable neighbors only
knn_spec <- nearest_neighbor(
  weight_func = "optimal",
  neighbors = tune()
) %>%
  set_mode("classification") %>%
  set_engine("kknn")

# HDLSS = High-Dimensional, Low Sample Size
# This dataset (n = 1000, p = 89) is HDLSS-adjacent — dimensionality is moderate, 
# so KNN may still be unstable for low k

# Define and finalize the KNN hyperparameter space for this setting
knn_param_space <- extract_parameter_set_dials(knn_spec) %>%
  finalize(train_data) |>
  update(
    # Set a safe and empirically justified neighbor range
    # Avoid low k (e.g. 3 or 5), as it overfits in moderate/high-dimensional data
    # Start tuning from k = 7 (min stability threshold) to √n ≈ 31 (smoothed upper bound)
    neighbors = neighbors(c(7L, 31L))
  )


# Generate a smaller, efficient Latin hypercube grid for tuning
knn_grid <- grid_space_filling(knn_param_space, size = 50, type = "latin_hypercube")

# Build the workflow
knn_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(knn_spec)

# Step 1: Tune with grid search
knn_tune_grid <- tune_grid(
  knn_workflow,
  resamples = v_folds,
  grid = knn_grid,
  metrics = class_metrics,
  control = control_grid(save_pred = TRUE,verbose = T)
)

# Show top tuning results by MCC
best_knn_results <- show_best(knn_tune_grid, metric = "mcc")
print(best_knn_results)

# Select best hyperparameters
best_knn_params <- select_best(knn_tune_grid, metric = "mcc")

# Finalize workflow with best params and fit on full training data
final_knn_workflow <- finalize_workflow(knn_workflow, best_knn_params)

knn_fit <- fit(final_knn_workflow, data = train_data)

print(knn_fit)


# Save lighter model
#cleaned_KNN <- butcher(Knn_fit, verbose = TRUE)

```

## Results KNN

```{r plots for KKNN}
set.seed(RANDOM_SEED)

knn_res <- fit_resamples(
  final_knn_workflow,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

collect_metrics(knn_res)

# Extract predictions
predictions <- knn_fit %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Create a confusion matrix
confusion <- conf_mat(predictions,truth = Cell_type, estimate = .pred_class)
metrics_df <- summary(confusion)
metrics_df

# View the confusion matrix
autoplot(confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - KNN(8) HS Subtype Batch Heatmap ")

#write_csv(metrics_df, "KNN_tuned_metrics.csv")

```

# XGBoost (Extreme Gradient Boosting):

```{r Use xgb with CV}
library(xgboost)
library(bonsai)
set.seed(RANDOM_SEED)

#--- XGBoost Spec with Regularization and Tweaked Defaults ---
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  mtry = tune(),
  min_n = tune(),
  stop_iter = 10                 # for early stopping
) %>% 
  set_engine("xgboost", objective = "multi:softprob") |>
  #set_engine("lightgbm") %>%
  set_mode(mode = "classification" ) 

#--- Tunable Parameters ---
xgb_param_space <- extract_parameter_set_dials(xgb_spec) %>%
  update(
    trees = trees(c(500L, 1000)),
    tree_depth = tree_depth(c(3L, 10L)),
    learn_rate = learn_rate(c(0.01, 0.1)),  # fine-grained learning rate
    mtry = mtry(c(5L, 89L)),
    min_n = min_n(c(2L, 20L))
  )

#--- Grid Definition ---
xgb_grid <- grid_space_filling(
  xgb_param_space,
  size = 50,
  type = "latin_hypercube"
)

#--- Workflow Setup ---
xgb_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(xgb_spec) %>%
  add_case_weights(Weights)

#--- Grid Search (Initial) ---
xgb_tune_grid <- tune_grid(
  xgb_workflow,
  resamples = v_folds,
  grid = xgb_grid,
  metrics = class_metrics,
  control = control_grid(save_pred = TRUE)
)

#--- Bayesian Tuning (Refinement) ---
xgb_tune_bayes <- tune_bayes(
  xgb_workflow,
  resamples = v_folds,
  initial = xgb_tune_grid,
  param_info = xgb_param_space,
  iter = 25,
  metrics = class_metrics,
  control = control_bayes(
    no_improve = 10,
    verbose_iter = T,
    save_pred = TRUE,
    save_workflow = TRUE
  )
)

#--- Final Selection ---
xgb_best_params <- select_best(xgb_tune_bayes, metric = "mcc") #mcc

#--- Final Workflow ---
xgb_final_wf <- finalize_workflow(xgb_workflow, xgb_best_params)

#--- Final Fit on Training Data ---
xgb_fit <- fit(xgb_final_wf, data = train_data)
xgb_fit

# Save lighter model
#cleaned_XGB <- butcher(xgb_fit, verbose = TRUE)

```

## Results XGBoost

```{r plot xgb }
set.seed(RANDOM_SEED)

# Fit the model to resamples (this is what stacks needs)
XGB_res <- fit_resamples(
  xgb_final_wf,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

collect_metrics(XGB_res)

# Extract predictions
predictions <- xgb_fit %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Create a confusion matrix
confusion <- conf_mat(predictions,truth = Cell_type, estimate = .pred_class)
metrics_df <- summary(confusion)
metrics_df

# Visualize confusion matrix as a heatmap
autoplot(confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - XGB HS Subtype Batch Heatmap")

#write_csv(metrics_df,"XGB_tuned_metrics.csv")


```

# SVM
## SVM RBF

```{r}
set.seed(RANDOM_SEED)
#class_metrics <- metric_set(bal_accuracy,mcc, accuracy, f_meas, kap, roc_auc)
# Define the model specification with SVM and RBF kernel
svm_spec <- svm_rbf(
  cost = tune(),       # Cost parameter (C)
  rbf_sigma = tune(),   # Sigma parameter for RBF kernel
  margin = tune()
) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_grid <- extract_parameter_set_dials(svm_spec) %>%
  finalize(train_data |> select(-Cell_type)) %>%
  grid_space_filling(size = 50, type = "latin_hypercube")

# Create the workflow
svm_workflow <- 
  workflow() %>%
  add_recipe(recipe) %>%
  add_model(svm_spec)

# Perform initial grid tuning
tune_grid_results <- tune_grid(
  svm_workflow,
  resamples = v_folds, 
  grid = svm_grid,
  metrics = class_metrics,
  control = control_grid(save_pred = TRUE)
)

# Bayesian optimization tuning to refine the search
tune_bayes_results <- tune_bayes(
  svm_workflow,
  resamples = v_folds,
  iter = 25,              # Number of Bayesian iterations
  initial = tune_grid_results,
  metrics = class_metrics,
  control = control_bayes(no_improve = 5, verbose_iter = T)
)

# Create tuning visualization
autoplot(tune_bayes_results) +
  theme_minimal() +
  labs(title = "SVM with RBF Kernel - Hyperparameter Tuning Results")

# Select the best hyperparameters
best_params <- select_best(tune_bayes_results, metric = "mcc") # "mcc" "bal_accuracy"
print(best_params)

# Finalize the workflow with the best parameters
final_svm_rbf_workflow <- finalize_workflow(svm_workflow, best_params)

# Fit the model on training data
svm_rbf_fit <- final_svm_rbf_workflow %>%
  fit(data = train_data)

# Print summary of the fitted workflow
print(svm_rbf_fit)

# Save lighter model using butcher
#cleaned_svm_rbf <- butcher(svm_rbf_fit, verbose = TRUE)

```


### Metrics
```{r}
set.seed(RANDOM_SEED)

# Fit the model to resamples (this is what stacks needs)
SVM_rbf_res <- fit_resamples(
  final_svm_rbf_workflow,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

collect_metrics(SVM_rbf_res)

# Extract predictions from test data
predictions <- svm_rbf_fit %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Create a confusion matrix
confusion <- conf_mat(predictions, truth = Cell_type, estimate = .pred_class)
print(confusion)

# Get detailed metrics
metrics_df <- summary(confusion)
print(metrics_df)

# Visualize confusion matrix
autoplot(confusion, type = "heatmap") +
  #theme_void() +
  labs(title = "Confusion Matrix - SVM-RBF Subtype Batch Heatmap")

# Save metrics to CSV
#write_csv(metrics_df, "output/tables/SVM_RBF_tuned_metrics.csv")

```

## SVM linear

```{r}
set.seed(RANDOM_SEED)

# Define the model specification with SVM Linear kernel
svm_linear_spec <- svm_linear(
  cost = tune(),       # Cost parameter (C)
  margin = tune()      # Margin parameter
) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Use the Latin hypercube sampling approach you preferred
svm_grid <- extract_parameter_set_dials(svm_linear_spec) %>%
  finalize(train_data |> select(-Cell_type)) %>%
  grid_space_filling(size = 50, type = "latin_hypercube")

# Create the workflow
svm_workflow <- 
  workflow() %>%
  add_recipe(recipe) %>%
  add_model(svm_linear_spec) 

# Perform initial grid tuning
tune_grid_results <- tune_grid(
  svm_workflow,
  resamples = v_folds, 
  grid = svm_grid,
  metrics = class_metrics,
  control = control_grid(save_pred = TRUE)
)

# Show initial results
initial_results <- show_best(tune_grid_results, metric = "mcc", n = 5)
print(initial_results)

# Bayesian optimization tuning to refine the search
tune_bayes_results <- tune_bayes(
  svm_workflow,
  resamples = v_folds,
  iter = 25,              # Number of Bayesian iterations
  initial = tune_grid_results,
  metrics = class_metrics,
  control = control_bayes(no_improve = 5, verbose_iter = T)
)

# Collect and display final tuning results
tuning_results <- show_best(tune_bayes_results, metric = "mcc")
print(tuning_results)

# Create tuning visualization
autoplot(tune_bayes_results) +
  theme_minimal() +
  labs(title = "SVM with Linear Kernel - Hyperparameter Tuning Results")

# Select the best hyperparameters
best_params <- select_best(tune_bayes_results, metric = "mcc") # bal_accuracy mcc
print(best_params)

# Finalize the workflow with the best parameters
final_svm_linear_workflow <- finalize_workflow(svm_workflow, best_params)

# Fit the model on training data
svm_linear_fit <- final_svm_linear_workflow %>%
  fit(data = train_data)

# Print summary of the fitted workflow
print(svm_linear_fit)

# Save lighter model using butcher
#cleaned_svm_linear <- butcher(svm_linear_fit, verbose = TRUE) 

```


### Metrics
```{r}
# Extract predictions from test data
predictions <- svm_linear_fit %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Create a confusion matrix
confusion <- conf_mat(predictions, truth = Cell_type, estimate = .pred_class)
print(confusion)

# Get detailed metrics
metrics_df <- summary(confusion)
print(metrics_df)

# Visualize confusion matrix
autoplot(confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - SVM-Linear Subtype Batch Heatmap")

# Save metrics to CSV
#write_csv(metrics_df, "output/tables/SVM_Linear_tuned_metrics.csv")

# Fit the model to resamples (this is what stacks needs)
SVM_linear_res <- fit_resamples(
  final_svm_linear_workflow,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

collect_metrics(SVM_linear_res)

```

# LASSO

```{r}

set.seed(RANDOM_SEED)

# Define the model specification with LASSO
lasso_spec <- multinom_reg(
  penalty = tune(),      # Regularization parameter
  mixture = 1          # 1 = LASSO, 0 = Ridge, values between = Elastic Net
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

# Use the Latin hypercube sampling approach for the penalty parameter
lasso_grid <- extract_parameter_set_dials(lasso_spec) %>%
  finalize(train_data) %>%  # Uncomment if needed for specific values
  grid_space_filling(size = 50, type = "latin_hypercube")

# Create the workflow
lasso_workflow <- 
  workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec) |>   add_case_weights(col = Weights)

# Perform initial grid tuning
tune_grid_results <- tune_grid(
  lasso_workflow,
  resamples = v_folds, 
  grid = lasso_grid,
  metrics = class_metrics,
  control = control_grid(save_pred = TRUE)
)

# Show initial results
initial_results <- show_best(tune_grid_results, metric = "mcc", n = 5)
print(initial_results)

# Bayesian optimization tuning to refine the search
tune_bayes_results <- tune_bayes(
  lasso_workflow,
  resamples = v_folds,
  iter = 25,              # Number of Bayesian iterations
  initial = tune_grid_results,
  metrics = class_metrics,
  control = control_bayes(no_improve = 5, verbose_iter = T)
)

# Collect and display final tuning results
tuning_results <- show_best(tune_bayes_results, metric = "mcc", n = 10)
print(tuning_results)

# Create tuning visualization
autoplot(tune_bayes_results) +
  theme_minimal() +
  labs(title = "LASSO Model - Hyperparameter Tuning Results")

# Select the best hyperparameters
best_params <- select_best(tune_bayes_results, metric = "mcc")
print(best_params)

# Finalize the workflow with the best parameters
final_lasso_workflow <- finalize_workflow(lasso_workflow, best_params)

# Fit the model on training data
lasso_fit <- final_lasso_workflow %>%
  fit(data = train_data)

# Print summary of the fitted workflow
print(lasso_fit)

# Save lighter model using butcher
cleaned_lasso <- butcher(lasso_fit, verbose = TRUE) 

```


## Metrics
```{r}

# Extract predictions from test data
predictions <- lasso_fit %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Create a confusion matrix
confusion <- conf_mat(predictions, truth = Cell_type, estimate = .pred_class)
print(confusion)

# Get detailed metrics
metrics_df <- summary(confusion)
print(metrics_df)

# Visualize confusion matrix
autoplot(confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - LASSO Subtype Batch Heatmap")

# Save metrics to CSV
#write_csv(metrics_df, "output/tables/LASSO_tuned_metrics.csv")

# Fit the model to resamples (this is what stacks needs)
LASSO_res <- fit_resamples(
  final_lasso_workflow,
  resamples = v_folds,
  metrics = class_metrics,
  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
)

```


# STACK models
```{r}
# Now you can use this for stacking
# First, load the stacks library if not already loaded
set.seed(RANDOM_SEED)
library(stacks)

# Initialize a model stack
model_stack <- stacks() %>%
  add_candidates(LASSO_res) %>%
  add_candidates(SVM_linear_res) %>%
  add_candidates(SVM_rbf_res) %>%
  add_candidates(RF_res) %>%
  add_candidates(XGB_res) %>%
  blend_predictions(
    metric = metric_set(kap),
    times = 300,
    mixture = 0.8,
    control = control_grid(
      verbose = TRUE,
      save_pred = TRUE          
    )
  ) %>%
  fit_members()

#saveRDS(object = model_stack,file = "model_stack.RDS")
print(model_stack)
collect_parameters(model_stack, "RF_res")
collect_parameters(model_stack, "SVM_rbf_res")
collect_parameters(model_stack, "SVM_linear_res")
# Visualize the model weights
autoplot(model_stack, type = "performance")
autoplot(model_stack, type = "weights")
autoplot(model_stack, type = "members")




# Now you can make predictions with the stack on new data
stack_preds <- model_stack %>%
  predict(new_data = test_data) %>%
  mutate(Cell_type = test_data$Cell_type)

# Evaluate stack performance
stack_confusion <- conf_mat(stack_preds, truth = Cell_type, estimate = .pred_class)
stack_metrics <- summary(stack_confusion)
stack_metrics

# Visualize confusion matrix for the stacked model
autoplot(stack_confusion, type = "heatmap") +
  labs(title = "Confusion Matrix - Stacked Model for HS Subtype Classification")

# Save metrics
#write_csv(stack_metrics, "stacked_model_metrics.csv")
#Cleaned_stack <- butcher(model_stack,verbose = T)

```


# Dotplot metrics

```{r prepare_metrics}
library(tidyverse)

# Define a function to process metrics files
process_metrics <- function(file, model_name) {
  read_csv(file) %>%
    select(-.estimator) %>%
    t() %>%
    as.data.frame() %>%
    `rownames<-`(NULL) %>%
    setNames(.[1, ]) %>%
    slice(-1) %>%
    mutate(Model = model_name) %>%
    as_tibble()
}

# List of all metrics files and their corresponding model names
metrics_files <- list(
  "output/tables/Tailored_RF_metrics.csv" = "RF",
  #"output/tables/Mlp_tuned_metrics.csv" = "MLP",
  "output/tables/Tailored_KKNN_metrics.csv" = "KNN(7)",
  "output/tables/Tailored_XGB_metrics.csv" = "XGBoost",
  "output/tables/Tailored_SVMrbf_metrics.csv" = "SVM RBF",
  "output/tables/Tailored_SVMlinear_metrics.csv" = "SVM Linear",
  "output/tables/Tailored_LASSO_metrics.csv" = "LASSO",
  "output/tables/Tailored_Stack_metrics.csv" = "STACKED (all models)"
)

# Process all files and combine
all_metrics <- map2_dfr(names(metrics_files), metrics_files, process_metrics) %>%
  mutate(across(c(bal_accuracy, kap, f_meas, mcc), as.numeric)) %>%
  select(-ppv) |>
  mutate(UN_mcc = (mcc + 1) / 2) |>
  select(Model, mcc, everything()) %>%
  arrange(desc(mcc)) %>%
  mutate(across(where(is.numeric), ~ signif(.x, 3)))

all_metrics

# Save results
#write_csv(all_metrics, "output/tables/Metrics_HS_Subtype.csv")

```

```{r}
library(ggsci)

# Your data wrangling
plot_data <- all_metrics %>%
  select(Model, f_meas, mcc, bal_accuracy) %>%
  mutate(Model = reorder(Model, mcc)) %>%  # Order by MCC
  pivot_longer(cols = c(f_meas, mcc, bal_accuracy),
               names_to = "Metric", values_to = "Value") %>%
  mutate(Label = sprintf("%.2f", Value)) %>%
  mutate(Metric = recode(Metric,
                         "f_meas" = "F1 Score",
                         "mcc" = "Matthews Correlation Coefficient",
                         "bal_accuracy" = "Balanced Accuracy"))

# Plot
HS_simple_bar <- ggplot(plot_data, aes(x = Model, y = Value, fill = Metric)) +
  geom_col(position = position_dodge(0.7), width = 0.6) +
  geom_text(aes(label = Label), position = position_dodge(0.7), vjust = -0.5, size = 4.5) +
  scale_fill_npg() +
  labs(title = "HS Subtype Models Performances", 
       y = "Score", 
       fill = "Metric") +
  theme_bw(base_size = 15) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        legend.position = "top") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2))

print(HS_simple_bar)

#ggsave("output/figures/HS_simple_bar_Sub.png", plot = HS_simple_bar, width = 12, height = 8, dpi = 600, bg = "white")

```

# Utilities
## Calibration
```{r}

calibrate_and_save <- function(model, test_data, model_name,method) {
  library(tailor)
  library(probably)
  library(dplyr)
  library(readr)
  library(ggplot2)
  
  set.seed(RANDOM_SEED)
  
  # Predict class and probabilities
  class_preds <- predict(model, new_data = test_data, type = "class")
  prob_preds <- predict(model, new_data = test_data, type = "prob")
  
  # Combine predictions and true labels
  predictions <- bind_cols(class_preds, prob_preds) %>%
    mutate(truth = test_data$Cell_type) %>%
    rename(predicted = .pred_class)
  
  # Build and fit calibration model (e.g., isotonic)
  post_obj <- tailor() %>% adjust_probability_calibration(method = method)
  post_res <- fit(post_obj, predictions, outcome = truth, estimate = predicted, probabilities = starts_with(".pred_"))
  
  # Get calibrated probabilities
  calibrated_probs <- predict(post_res, predictions, type = "prob")
  
  # Confusion matrix and metrics summary
  confusion <- conf_mat(calibrated_probs, truth = truth, estimate = predicted)
  metrics_df <- summary(confusion)
  print(metrics_df)
  # Plot confusion heatmap
  print(autoplot(confusion, type = "heatmap"))
  
  # Save metrics and calibration model files
  write_csv(metrics_df, file = paste0("output/tables/Tailored_", model_name, "_metrics.csv"))
  saveRDS(post_res, file = paste0("output/objects/", model_name, "_tailor.RDS"))
  
  # Return metrics and calibration model object
  list(metrics = metrics_df, calibration_model = post_res)
}

# isotonic beta

NB_results <- calibrate_and_save(nb_final_fit, test_data, "NB","isotonic")

AORSF_results <- calibrate_and_save(aorsf_final_fit, test_data, "AORSF","beta")

AORSF_results <- calibrate_and_save(aorsf_final_fit, test_data, "C5_0","isotonic_boot")

XGB_results <- calibrate_and_save(xgb_fit, test_data, "XGB","isotonic")

RF_results <- calibrate_and_save(rf_fit, test_data, "RF","isotonic_boot")
#results <- calibrate_and_save(xgb_fit, test_data, "XGB")

SVMrbf_results <- calibrate_and_save(svm_rbf_fit, test_data, "SVMrbf","isotonic_boot")

SVMlinear_results <- calibrate_and_save(svm_linear_fit, test_data, "SVMlinear","isotonic_boot")

LASSO_results <- calibrate_and_save(lasso_fit, test_data, "LASSO","isotonic")

KKNN_results <- calibrate_and_save(knn_fit, test_data, "KKNN","isotonic_boot")

Stack_results <- calibrate_and_save(model_stack, test_data, "Stack","isotonic_boot")




```

# SHAP 
## RF
```{r}
library(ranger)
library(kernelshap)
library(rsample)
library(dplyr)
library(doFuture)

set.seed(1)

# --- Extract the fitted ranger model ---
rf_model <- extract_fit_engine(rf_fit)

# --- Prepare full feature matrix (exclude target and weights) ---
x <- train_data %>%
  select(-Cell_type, -Weights)

# --- Prepare background dataset using stratified sampling (~5% of training set) ---
stratified_subset <- train_data %>%
  initial_split(prop = 0.05, strata = Cell_type) %>%
  training() %>%
  select(-Cell_type, -Weights)

# --- Define prediction function returning probabilities for kernelshap ---
pred_fun <- function(object, newdata) {
  predict(object, data = newdata)$predictions
}

# --- Parallel backend setup ---
registerDoFuture()
plan(multisession, workers = 5)

# --- Run Kernel SHAP ---
SHAP_kernel_RF <- kernelshap(
  object = rf_model,
  X = x,
  bg_X = stratified_subset,
  pred_fun = pred_fun
)
# Step 2: Turn them into a shapviz object
SHAP_viz_RF <- shapviz(SHAP_kernel_RF, interactions = TRUE)
saveRDS(SHAP_viz_RF, "output/objects/SHAP_viz_RF_model.RDS")

shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 15) +
  ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)

shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
  theme_bw(base_size = 20) + 
  theme(legend.position = "bottom")
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype_5.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype_5.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)


# Save SHAP importance (beeswarm plot) as PNG with 600 DPI and a title
plot_beeswarm <- sv_importance(SHAP_viz_RF, show_numbers = TRUE, kind = "beeswarm") & theme_bw(base_size = 15)
plot_beeswarm
ggsave("SHAP/sv_importance_RF_Subtype_beeswarm.png", plot = plot_beeswarm, dpi = 600,width = 12,height = 8)

# Save SHAP dependence plot for 'EBF1' as PNG with 600 DPI
plot_dependence <- sv_dependence(SHAP_viz_RF, v = "EBF1", color_var = "auto") & theme_bw(base_size = 15) # & geom_smooth(method = lm)
plot_dependence
ggsave("SHAP/sv_dependence_RF_Subtype_EBF1.png", plot = plot_dependence, dpi = 600,width = 12,height = 8)

# Save SHAP waterfall plot as PNG with 600 DPI
plot_waterfall <- sv_waterfall(SHAP_viz_RF) +
  ggtitle("SHAP waterfall RF Subtype") & theme_bw(base_size = 15)
plot_waterfall
ggsave("SHAP/sv_waterfallRF_Subtype.png", plot = plot_waterfall, dpi = 600,width = 12,height = 8)

```

```{r}
library(fastshap)
library(ranger)

# 1. Extract the raw ranger model from your workflow fit
ranger_model <- extract_fit_engine(rf_fit)

# 2. Prepare prediction wrapper for fastshap
pred_wrapper <- function(object, newdata) {
  # predict probabilities (assuming classification)
  predict(object, data = newdata)$predictions
}

# 3. Calculate SHAP values on your baked data
set.seed(123)  # for reproducibility
shap_values <- fastshap::explain(
  object = ranger_model,
  feature_names = colnames(baked_data)[colnames(baked_data) != "Cell_type"],
  X = baked_data %>% select(-Cell_type),
  pred_wrapper = pred_wrapper, 
  adjust = TRUE,
  nsim = 100  # number of Monte Carlo simulations for SHAP approx
)

# 4. Inspect SHAP values (summary, plot, etc.)
head(shap_values)





```


## XBGoost
```{r}
library(xgboost)
library(kernelshap)
library(shapviz)

XGB <- xgb_fit %>% 
  extract_fit_engine()


# Prepare predictor matrix (same as used in model fitting)
x <- baked_data
x$Cell_type <- as.numeric(x$Cell_type) - 1

# Stratified small subset for readable axis labels (optional)
stratified_subset <- x %>%
  initial_split(prop = 0.05, strata = Cell_type) %>%  # ~5% stratified sample
  training() %>%
  select(-Cell_type)

# Matrix input for SHAP prediction (must match training format)
X_pred <- data.matrix(x %>% select(-Cell_type, -Weights))

# Run SHAP
SHAP_viz_XGB <- shapviz(XGB, X_pred = X_pred)

# Save SHAP importance (default plot) as PNG with 600 DPI
SHAP_XGB <- sv_importance(SHAP_viz_XGB, show_numbers = TRUE,max_display = 25) +
  ggtitle("SHAP XGB importance subtype") + theme_bw(base_size = 15)

SHAP_XGB$data <- SHAP_XGB$data |>
  mutate(ind = recode(ind,
                      "Class_1" = "B",
                      "Class_2" = "PreB",
                      "Class_3" = "PreT",
                      "Class_4" = "T"))
  

SHAP_XGB

ggsave(plot = SHAP_XGB,filename = "output/figures/SHAP_XGB_total.png", width = 12,height = 8,dpi = 600)

SHAP_XGB_list <- SHAP_XGB[["data"]] |>
  filter(ind == "B") |>
  select(feature) |>
  tibble()

SHAP_XGB

write_csv(x = SHAP_XGB,file = "output/tables/SHAP_XGB_list.csv")
```

## MLP

```{r}
set.seed(RANDOM_SEED)
library(baguette)

x <- train_data[-ncol(train_data)]
stratified_subset <- train_data %>%
  initial_split(prop = 100 / nrow(train_data), strata = Cell_type) %>%
  training() |>
  select(-Cell_type)

SHAP_kernel_NNET <- kernelshap(mlp_SHAP_fit, x, bg_X = stratified_subset,parallel = F,type = "prob")

# Step 2: Turn them into a shapviz object
SHAP_viz_NNET <- shapviz(SHAP_kernel_NNET, interactions = TRUE)

# Save SHAP importance (beeswarm plot) as PNG with 600 DPI and a title
plot_beeswarm <- sv_importance(SHAP_viz_NNET, show_numbers = TRUE, kind = "beeswarm") + theme_bw(base_size = 20)
plot_beeswarm
ggsave("SHAP/sv_importance_MLP_Subtype_beeswarm.png", plot = plot_beeswarm, dpi = 600,width = 12,height = 8)

# Save SHAP importance (default plot) as PNG with 600 DPI
plot_default <- sv_importance(SHAP_viz_NNET, show_numbers = TRUE) +
  ggtitle("SHAP Importance MLP Subtype") + theme_bw(base_size = 20)
plot_default
ggsave("SHAP/sv_importance_MLP_Subtype_default.png", plot = plot_default, dpi = 600,width = 12,height = 8)

# Save SHAP dependence plot for 'EBF1' as PNG with 600 DPI
plot_dependence <- sv_dependence(SHAP_viz_NNET, v = "EBF1", color_var = "auto") + theme_bw(base_size = 20)
plot_dependence
ggsave("SHAP/sv_dependence_MLP_Subtype_EBF1.png", plot = plot_dependence, dpi = 600,width = 12,height = 8)

# Save SHAP waterfall plot as PNG with 600 DPI
plot_waterfall <- sv_waterfall(SHAP_viz_NNET) + theme_bw(base_size = 20)
plot_waterfall
ggsave("SHAP/sv_waterfall_MLP_Subtype.png", plot = plot_waterfall, dpi = 600,width = 12,height = 8)


```


## STACK

```{r}
library(stacks)
set.seed(RANDOM_SEED)
# Predict class labels from the stack
stack_preds_train <- predict(model_stack, new_data = train_data) %>%
  mutate(Cell_type_stack = .pred_class)

# Merge with original features
surrogate_data <- train_data %>%
  mutate(Cell_type_stack = stack_preds_train$Cell_type_stack) |>
  select(-Cell_type)

head(surrogate_data)
```

```{r}
library(tidymodels)
library(xgboost)
library(bonsai)
library(vip)
library(stacks)

set.seed(RANDOM_SEED)

# Step 1: Get predictions from your stacked model
stack_preds_train <- predict(model_stack, new_data = train_data) %>%
  mutate(Cell_type_stack = .pred_class)

# Step 2: Create surrogate dataset with stack predictions as target
surrogate_data <- train_data %>%
  mutate(Cell_type_stack = stack_preds_train$Cell_type_stack) %>%
  select(-Cell_type, -Weights)

# Step 3: Create recipe for surrogate model (can reuse your existing recipe)
surrogate_recipe <- recipe(Cell_type_stack ~ ., data = surrogate_data)

# Step 4: XGBoost specification for surrogate model
surrogate_xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  learn_rate = tune(),
  mtry = tune(),
  min_n = tune(),stop_iter = 50
) %>%
  set_engine("xgboost", objective = "multi:softprob") %>%
  set_mode("classification")

# Step 5: Parameter space (you can use smaller ranges for surrogate)
surrogate_param_space <- extract_parameter_set_dials(surrogate_xgb_spec) %>%
  update(
    trees = trees(c(100L, 500L)),        # Fewer trees for surrogate
    tree_depth = tree_depth(c(3L, 8L)),
    learn_rate = learn_rate(c(0.01, 0.2)),
    mtry = mtry(c(5L, 50L)),             # Adjust based on your feature count
    min_n = min_n(c(2L, 15L))
  )

# Step 6: Smaller grid for surrogate tuning
surrogate_grid <- grid_space_filling(
  surrogate_param_space,
  size = 50,  # Smaller grid since this is just for interpretability
  type = "latin_hypercube"
)

# Step 7: Workflow setup
surrogate_workflow <- workflow() %>%
  add_recipe(surrogate_recipe) %>%
  add_model(surrogate_xgb_spec)
  # Note: Remove case weights if they're not needed for surrogate

# Step 8: Create resamples for surrogate data
surrogate_folds <- vfold_cv(surrogate_data, strata = Cell_type_stack,v = 5, repeats = 3)

# Step 9: Quick grid search (no need for extensive tuning)
surrogate_tune <- tune_grid(
  surrogate_workflow,
  resamples = surrogate_folds,
  grid = surrogate_grid,
  metrics = class_metrics,  # Simpler metrics for surrogate
  control = control_grid(save_pred = TRUE)
)

# Step 10: Select best parameters and finalize
surrogate_best <- select_best(surrogate_tune, metric = "mcc")
surrogate_final_wf <- finalize_workflow(surrogate_workflow, surrogate_best)

# Step 11: Fit final surrogate model
surrogate_fit <- fit(surrogate_final_wf, data = surrogate_data)

# Step 12: Check surrogate model performance
surrogate_preds <- predict(surrogate_fit, new_data = test_data ) %>% #surrogate_data
  bind_cols(test_data)

surrogate_accuracy <- bal_accuracy(surrogate_preds, Cell_type, .pred_class)
cat("Surrogate model balanced accuracy:", surrogate_accuracy$.estimate, "\n")
surrogate_accuracy <- mcc(surrogate_preds, Cell_type, .pred_class)
cat("Surrogate model MCC:", surrogate_accuracy$.estimate, "\n")
surrogate_accuracy <- f_meas(surrogate_preds, Cell_type_stack, .pred_class)
cat("Surrogate model F1:", surrogate_accuracy$.estimate, "\n")

```


```{r}

# Step 13: Extract feature importance
# Method 1: Variable importance plot
importance_plot <- surrogate_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 20) +
  labs(title = "Surrogate Model Feature Importance",
       subtitle = "Top features explaining stacked model predictions")

print(importance_plot)

# Step 14: Optional - SHAP values for more detailed interpretation
library(shapviz)

# Extract the underlying xgboost model
xgb_model <- surrogate_fit %>% 
  extract_fit_parsnip() %>% 
  extract_fit_engine()

# Prepare data for SHAP (need to process through recipe)
processed_data <- surrogate_recipe %>%
  prep() %>%
  bake(new_data = surrogate_data %>% select(-Cell_type_stack))

# Create SHAP explainer
shap_values <- shapviz(xgb_model, X_pred = as.matrix(processed_data))

# Step 15: Save results
# Save the surrogate model
saveRDS(surrogate_fit, "output/objects/surrogate_model.RDS")
saveRDS(shap_values, "output/objects/shap_values_surrogate_model.RDS")

```


```{r}

# Save SHAP importance (default plot) as PNG with 600 DPI
SHAP_STACK <- sv_importance(shap_values, show_numbers = TRUE,max_display = 25) +
  ggtitle("SHAP Stack importance subtype") + theme_bw(base_size = 15)

SHAP_STACK$data <- SHAP_STACK$data |>
  mutate(ind = recode(ind,
                      "Class_1" = "B",
                      "Class_2" = "PreB",
                      "Class_3" = "PreT",
                      "Class_4" = "T"))
  

SHAP_STACK

ggsave(plot = SHAP_STACK,filename = "output/figures/Total_SHAP_stack.png", width = 10,height = 6,dpi = 600)

SHAP_STACK_top4 <- sv_importance(shap_values, show_numbers = TRUE,max_display = 4) +
  ggtitle("SHAP Stack importance subtype") + theme_bw(base_size = 15)

SHAP_STACK_top4$data <- SHAP_STACK_top4$data |>
  mutate(ind = recode(ind,
                      "Class_1" = "B",
                      "Class_2" = "PreB",
                      "Class_3" = "PreT",
                      "Class_4" = "T"))
  

SHAP_STACK_top4

ggsave(plot = SHAP_STACK_top4,filename = "output/figures/Total_SHAP_stack_4.png", width = 10,height = 6,dpi = 600)


SHAP_STACK_beeswarm <- sv_importance(sv, show_numbers = TRUE, kind = "beeswarm",) & theme_bw(base_size = 10)
labels <- c("B", "PreB", "PreT", "T")
SHAP_STACK_beeswarm[[1]]$labels$title <- "B"
SHAP_STACK_beeswarm[[2]]$labels$title <- "PreB"
SHAP_STACK_beeswarm[[3]]$labels$title <- "PreT"
SHAP_STACK_beeswarm[[4]]$labels$title <- "T"
# Now, `plot_beeswarm` should reflect the new titles per plot
print(SHAP_STACK_beeswarm)

ggsave(plot = SHAP_STACK_beeswarm,filename = "output/figures/SHAP_STACK_beeswarm.png", width = 12,height = 8,dpi = 600)

SHAP_STACK_list <- SHAP_STACK[["data"]] |>
  filter(ind == "B") |>
  select(feature) |>
  tibble()

SHAP_STACK_list

write_csv(x = SHAP_STACK_list, file = "output/tables/SHAP_STACK_list_25.csv")

# VIP
vip(xgb_surrogate, num_features = 20) + theme_bw(base_size = 12)

importance_matrix <- xgb.importance(model = xgb_surrogate )
xgb.ggplot.importance(importance_matrix = importance_matrix, top_n = 25, n_clusters = 3,rel_to_first = F) + 
  ggtitle("Feature Importance XGBoost HS Subtype Batch") +
  theme_bw(15)

```





# SHAP extraction

```{r BAR_plot}
plot_default_NNET <- sv_importance(SHAP_viz_NNET, show_numbers = TRUE,max_display = 30,color_bar_title = "Subtype") +
  ggtitle("SHAP Importance RF Subtype")
plot_default_NNET 

plot_default_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 30,color_bar_title = "Subtype") +
  ggtitle("SHAP Importance RF Subtype")

plot_default_XGB <- sv_importance(SHAP_viz_XGB, show_numbers = TRUE,max_display = 30,color_bar_title = "Subtype") +
  ggtitle("SHAP Importance RF Subtype")
```

##  Extract the values

```{r  Extract the values}
# Define the function to process model data
process_model_data <- function(data, model_name, recode_kind = FALSE) {
  # If recoding 'kind' is needed
  if (recode_kind) {
    data <- data %>%
      mutate(ind = recode(ind,
                           "Class_1" = "B",
                           "Class_2" = "PreB",
                           "Class_3" = "PreT",
                           "Class_4" = "T"))
  }
  
  # Split the data based on 'ind' and rename 'feature' column with model name
  data %>%
      group_by(ind) %>%
      group_split() %>%
      setNames(unique(data$ind)) %>%
      map2(., names(.), ~ .x %>%
              select(feature) %>%
              rename(
                  !!paste0(model_name, "_", .y, "_feature") := feature#,
                  #!!paste0(model_name, "_", .y, "_values") := values
              ))
}

# Apply the function to NNET, RF, and XGB datasets

# NNET list - recode 'kind'
NNET_list <- process_model_data(tibble(plot_default_NNET$data) %>%
                                  mutate(ind = str_replace(ind, ".*_", "")), "NNET", recode_kind = F)

# RF list - no 'kind' recoding
RF_list <- process_model_data(tibble(plot_default_RF$data), "RF")

# XGB list - recode 'kind'
XGB_list <- process_model_data(tibble(plot_default_XGB$data), "XGB", recode_kind = TRUE)


```

## cbind the data 

```{r}

concatenate_model_data_by_index <- function(RF_list, NNET_list, XGB_list) {
  pmap(list(RF_list, NNET_list, XGB_list), cbind)
}

# Example usage
combined_data_list <- data.frame(concatenate_model_data_by_index(RF_list, NNET_list, XGB_list))

```

## Use RankAggreg with CE to obtain the best list with score 

```{r}
library(RankAggreg)
weights <- c(3,1,2,3,1,2,3,1,2,3,1,2)
#combined_data_list <- data.matrix(combined_data_list)

top25CE <- RankAggreg(x = t(combined_data_list),k = 25, seed=100, method="CE", maxIter=3000, convIn=10,verbose = F,importance = weights)
top25CE
plot(top25CE)
title(main = "Top 25 list",line = 2.7)

top25 <- tibble(top25 = top25CE$top.list)

# Convert Ensembl IDs to gene names
gene_names <- gconvert(query = top25$top25, organism = "hsapiens", 
         target = "ENSG", mthreshold = Inf, filter_na = TRUE)

gene_names <- gene_names %>% 
  distinct(input, .keep_all = TRUE)
top25 <- top25 %>%
  mutate( nnsembl = unique(gene_names$target) ) %>%
  mutate( description = unique(gene_names$description) )
write_csv(top25, "Top25_HS_Subtype_total.csv")



```

# ALLcatchR

```{r}

library(ALLCatchRbcrabl1)
out <- allcatch_bcrabl1()

out <- allcatch_bcrabl1(Counts.file = t(ML_data) )

write.tsv(t(ML_data), "../../ALLDEGs/my_data.tsv")

my_data <- read_tsv("../../ALLDEGs/my_data.tsv")

my_data <- t(ML_data)

fwrite(my_data <- as.data.frame(t(ML_data)), "../../ALLDEGs/my_data.csv",row.names = T)


out <- allcatch_bcrabl1(Counts.file = "../../ALLDEGs/my_data.csv", ID_class = "symbol", sep = ",", out.file = "predictions.tsv")

out <- allcatch_bcrabl1(Counts.file = my_data, ID_class = "symbol", sep = ",", out.file = "predictions.tsv")

```



