geom_point(size = 4) +
geom_text_repel(
data = filter(plot_data_line, data == "HS"),
aes(label = Model),
size = 4,
nudge_x = 0.05,  # tighter nudge
direction = "y",
hjust = 0,
show.legend = FALSE,
segment.size = 0.3
) +
scale_color_npg() +
scale_y_continuous(limits = c(0.3, 1), breaks = seq(0, 1, by = 0.05)) +
scale_x_discrete(expand = expansion(mult = c(0.01, 0.05))) +  # tighten x-axis spacing
labs(
title = "Performance Models on Subtype Data",
x = "DEGs",
y = "Performance"
) +
theme_bw(base_size = 14) +
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
axis.title.x = element_text(size = 16, face = "bold"),
axis.title.y = element_text(size = 16, face = "bold"),
axis.text.x = element_text(size = 14, face = "bold"),
axis.ticks.length = unit(-0.2, "cm"),
panel.grid.major.y = element_line(color = "lightgray", linewidth = 0.2),
panel.grid.minor.y = element_blank(),
plot.margin = unit(c(20, 40, 40, 40), "pt")
)
ggsave("output/figures/HS_vs_woHS_lineplot_better_mcc.png", plot = lineplot_better,
width = 8, height = 5, dpi = 600, bg = "white")
ggsave("output/figures/HS_vs_woHS_lineplot_better_mcc.png", plot = lineplot_better,
width = 8, height = 5, dpi = 600, bg = "white")
print(lineplot_better)
plot_data_line <- mixed_metrics %>%
select(Model, mcc, data) %>%
mutate(mcc = as.numeric(mcc),
Model = factor(Model))
plot_data_line$data <- factor(x = plot_data_line$data, levels = c("nonHS","HS"))
lineplot_better <- ggplot(plot_data_line, aes(data, mcc, group = Model, color = Model)) +
geom_line(linewidth = 1) +
geom_point(size = 4) +
geom_text_repel(data = filter(plot_data_line, data == "HS"),
aes(label = Model), size = 4, nudge_x = 0.15,
direction = "y", hjust = 0, show.legend = FALSE) +
scale_color_npg() +
scale_y_continuous(limits = c(0.3, 1), breaks = seq(0, 1, by = 0.05)) + # Grid/ticks every 0.2 (adjustable)
scale_x_discrete(expand = expansion(mult = c(0.1, 0.2))) +
labs(
title = "Performance Models on Subtype Data",
x = "DEGs",
y = "Performance"
) +
theme_bw(14) +
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
axis.title.x = element_text(size = 16,face = "bold"),
axis.title.y = element_text(size = 16,face = "bold"),
axis.text.x = element_text(size = 14,face = "bold"),
axis.ticks.length = unit(-0.2, "cm"),
panel.grid.major.y = element_line(color = "lightgray", linewidth = 0.2),
panel.grid.minor.y = element_blank(),
plot.margin = unit(c(20, 40, 40, 40), "pt")
)
print(lineplot_better)
ggsave("output/figures/HS_vs_woHS_lineplot_better_mcc.png", plot = lineplot_better,
width = 10, height = 6, dpi = 600, bg = "white")
plot_data_line <- mixed_metrics %>%
select(Model, mcc, data) %>%
mutate(mcc = as.numeric(mcc),
Model = factor(Model))
plot_data_line$data <- factor(x = plot_data_line$data, levels = c("nonHS","HS"))
lineplot_better <- ggplot(plot_data_line, aes(data, mcc, group = Model, color = Model)) +
geom_line(linewidth = 1) +
geom_point(size = 4) +
geom_text_repel(data = filter(plot_data_line, data == "HS"),
aes(label = Model), size = 4, nudge_x = 0.15,
direction = "y", hjust = 0, show.legend = FALSE) +
scale_color_npg() +
scale_y_continuous(limits = c(0.3, 1), breaks = seq(0, 0.9, by = 0.05)) + # Grid/ticks every 0.2 (adjustable)
scale_x_discrete(expand = expansion(mult = c(0.1, 0.2))) +
labs(
title = "Performance Models on Subtype Data",
x = "DEGs",
y = "Performance"
) +
theme_bw(14) +
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
axis.title.x = element_text(size = 16,face = "bold"),
axis.title.y = element_text(size = 16,face = "bold"),
axis.text.x = element_text(size = 14,face = "bold"),
axis.ticks.length = unit(-0.2, "cm"),
panel.grid.major.y = element_line(color = "lightgray", linewidth = 0.2),
panel.grid.minor.y = element_blank(),
plot.margin = unit(c(20, 40, 40, 40), "pt")
)
print(lineplot_better)
ggsave("output/figures/HS_vs_woHS_lineplot_better_mcc.png", plot = lineplot_better,
width = 10, height = 6, dpi = 600, bg = "white")
install.packages("geomtextpath")
library(geomtextpath)
plot_data_line <- mixed_metrics %>%
select(Model, mcc, data) %>%
mutate(mcc = as.numeric(mcc),
Model = factor(Model))
plot_data_line$data <- factor(x = plot_data_line$data, levels = c("nonHS", "HS"))
lineplot_better <- ggplot(plot_data_line, aes(x = data, y = mcc, group = Model, color = Model)) +
geom_line(linewidth = 1) +
geom_point(size = 4) +
geom_textline(aes(label = Model),
hjust = 0.1,
linewidth = 1,
size = 4,
vjust = -0.5,
straight = TRUE,
show.legend = FALSE) +
scale_color_npg() +
scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3, 1, by = 0.05)) +
scale_x_discrete(expand = expansion(mult = c(0.1, 0.2))) +
labs(
title = "Performance Models on Subtype Data",
x = "DEGs",
y = "Performance"
) +
theme_bw(14) +
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5, face = "bold", size = 20),
axis.title.x = element_text(size = 16, face = "bold"),
axis.title.y = element_text(size = 16, face = "bold"),
axis.text.x = element_text(size = 14, face = "bold"),
axis.ticks.length = unit(-0.2, "cm"),
panel.grid.major.y = element_line(color = "lightgray", linewidth = 0.2),
panel.grid.minor.y = element_blank(),
plot.margin = unit(c(20, 40, 40, 40), "pt")
)
print(lineplot_better)
ggsave("output/figures/HS_vs_woHS_lineplot_better_mcc.png", plot = lineplot_better,
width = 10, height = 6, dpi = 600, bg = "white")
save.image("F:/ML/HS/Subtype_batch/output/objects/for metrics plots.RData")
load("F:/ML/HS/Subtype_batch/output/objects/DRAFT_MODELS_HS_SUBTYPE.RData")
# Define the vector of package names needed for the project
PACKAGES <- c(
"xgboost",         # Extreme Gradient Boosting algorithm for classification/regression
"ggplot2",         # Data visualization package based on the grammar of graphics
"RColorBrewer",    # Color palettes for enhanced data visualization
"tidymodels",      # Tidy modeling framework for machine learning workflows
"tidyverse",       # Collection of packages for data manipulation, analysis, and visualization
"finetune",        # Tools for model tuning and optimization
"themis",          # Techniques for handling class imbalance in machine learning
"gprofiler2",      # Gene enrichment and pathway analysis for gene lists
"future",          # Enables parallel processing for faster computations
"data.table",      # High-performance data manipulation package
"gt",              # For creating display tables
#"gtsummary",       # For summary tables with statistics and formatted output
"butcher",         # Reduces memory load of models by removing unnecessary objects
"UBL",              # Utility-based learning techniques for imbalanced data
"ranger",
"kernelshap",
"shapviz"
)
# Use purrr::walk to load all packages
purrr::walk(PACKAGES, library, character.only = TRUE)
gc()
plan(multisession, workers = 10)
nbrOfWorkers()
RANDOM_SEED <- 1234
#plan("multisession", workers = 15,)
#plan(multisession, workers = 15)
#plan(sequential)
library(ranger)
set.seed(1)
x <- train_data[-ncol(train_data)]
stratified_subset <- train_data %>%
initial_split(prop = 100 / nrow(train_data), strata = Cell_type) %>%
training() |>
select(-Cell_type)
# Step 1: Calculate Kernel SHAP values
library(doFuture)
registerDoFuture()
plan(multisession, workers = 5)
SHAP_kernel_RF <- kernelshap(extract_fit_engine(rf_fit), x, bg_X = stratified_subset)
library(ranger)
set.seed(1)
x <- train_data[-ncol(train_data)]
stratified_subset <- train_data %>%
initial_split(prop = 100 / nrow(train_data), strata = Cell_type) %>%
training() |>
select(-Cell_type, -Weights)
# Step 1: Calculate Kernel SHAP values
library(doFuture)
registerDoFuture()
plan(multisession, workers = 5)
SHAP_kernel_RF <- kernelshap(extract_fit_engine(rf_fit), x, bg_X = stratified_subset)
library(ranger)
library(kernelshap)
library(rsample)
library(dplyr)
library(doFuture)
set.seed(1)
# --- Extract the fitted ranger model ---
rf_model <- extract_fit_engine(rf_fit)
# --- Prepare full feature matrix (exclude target and weights) ---
x <- train_data %>%
select(-Cell_type, -Weights)
# --- Prepare background dataset using stratified sampling (~5% of training set) ---
stratified_subset <- train_data %>%
initial_split(prop = 0.05, strata = Cell_type) %>%
training() %>%
select(-Cell_type, -Weights)
# --- Define prediction function returning probabilities for kernelshap ---
pred_fun <- function(object, newdata) {
predict(object, data = newdata)$predictions
}
# --- Parallel backend setup ---
registerDoFuture()
plan(multisession, workers = 5)
# --- Run Kernel SHAP ---
SHAP_kernel_RF <- kernelshap(
object = rf_model,
X = x,
bg_X = stratified_subset,
pred_fun = pred_fun
)
library(ranger)
library(kernelshap)
library(rsample)
library(dplyr)
library(doFuture)
set.seed(1)
# --- Extract the fitted ranger model ---
rf_model <- extract_fit_engine(rf_fit)
# --- Prepare full feature matrix (exclude target and weights) ---
x <- train_data %>%
select(-Cell_type, -Weights)
# --- Prepare background dataset using stratified sampling (~5% of training set) ---
stratified_subset <- train_data %>%
initial_split(prop = 0.05, strata = Cell_type) %>%
training() %>%
select(-Cell_type, -Weights)
# --- Define prediction function returning probabilities for kernelshap ---
pred_fun <- function(object, newdata) {
predict(object, data = newdata)$predictions
}
# --- Parallel backend setup ---
registerDoFuture()
plan(multisession, workers = 5)
# --- Run Kernel SHAP ---
SHAP_kernel_RF <- kernelshap(
object = rf_model,
X = x,
bg_X = stratified_subset,
pred_fun = pred_fun
)
# Step 2: Turn them into a shapviz object
SHAP_viz_RF <- shapviz(SHAP_kernel_RF, interactions = TRUE)
saveRDS(SHAP_viz_RF, "output/objects/SHAP_viz_RF_model.RDS")
# Save SHAP importance (beeswarm plot) as PNG with 600 DPI and a title
plot_beeswarm <- sv_importance(SHAP_viz_RF, show_numbers = TRUE, kind = "beeswarm") & theme_bw(base_size = 15)
plot_beeswarm
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 15) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("SHAP/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 15) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 15) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype_5.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype_5.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 15) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20)
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20) + theme(legend.position = "bottom")
shap_bar_RF
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
ggtitle("SHAP Importance RF Subtype") + theme_bw(base_size = 20) + theme(legend.position = "bottom")
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype_5.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype_5.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
shap_bar_RF <- sv_importance(SHAP_viz_RF, show_numbers = TRUE,max_display = 5) +
theme_bw(base_size = 20) +
theme(legend.position = "bottom")
shap_bar_RF
ggsave("output/figures/shap_bar_RF_Subtype_5.png", plot = shap_bar_RF, dpi = 600,width = 10,height = 6)
ggsave("output/figures/shap_bar_RF_Subtype_5.svg", plot = shap_bar_RF, device = svglite::svglite, width = 10, height = 6)
# Save SHAP importance (beeswarm plot) as PNG with 600 DPI and a title
plot_beeswarm <- sv_importance(SHAP_viz_RF, show_numbers = TRUE, kind = "beeswarm") & theme_bw(base_size = 15)
plot_beeswarm
# Save SHAP dependence plot for 'EBF1' as PNG with 600 DPI
plot_dependence <- sv_dependence(SHAP_viz_RF, v = "EBF1", color_var = "auto") & theme_bw(base_size = 15) # & geom_smooth(method = lm)
plot_dependence
# Save SHAP waterfall plot as PNG with 600 DPI
plot_waterfall <- sv_waterfall(SHAP_viz_RF) +
ggtitle("SHAP waterfall RF Subtype") & theme_bw(base_size = 15)
plot_waterfall
# Define the vector of package names needed for the project
PACKAGES <- c(
"xgboost",         # Extreme Gradient Boosting algorithm for classification/regression
"ggplot2",         # Data visualization package based on the grammar of graphics
"RColorBrewer",    # Color palettes for enhanced data visualization
"tidymodels",      # Tidy modeling framework for machine learning workflows
"tidyverse",       # Collection of packages for data manipulation, analysis, and visualization
"finetune",        # Tools for model tuning and optimization
"themis",          # Techniques for handling class imbalance in machine learning
"gprofiler2",      # Gene enrichment and pathway analysis for gene lists
"future",          # Enables parallel processing for faster computations
"data.table",      # High-performance data manipulation package
"gt",              # For creating display tables
#"gtsummary",       # For summary tables with statistics and formatted output
"butcher",         # Reduces memory load of models by removing unnecessary objects
"UBL",              # Utility-based learning techniques for imbalanced data
"ranger",
"kernelshap",
"shapviz"
)
# Use purrr::walk to load all packages
purrr::walk(PACKAGES, library, character.only = TRUE)
gc()
plan(multisession, workers = 10)
nbrOfWorkers()
RANDOM_SEED <- 1234
#plan("multisession", workers = 15,)
#plan(multisession, workers = 15)
#plan(sequential)
#| warning: false
# Load the data
# Read the CPM table, set the first column as row names, and transpose the data
ML_data <- fread("../../ALLDEGs/CPM_log_HS_tumorvscontrol_70.csv")
# Convert Ensembl IDs to gene names
gene_names <- gconvert(query = ML_data$V1, organism = "hsapiens",
target = "ENSG", mthreshold = Inf, filter_na = TRUE)
#ML_data$V1 == gene_names$target
ML_data$V1 <- gene_names$name
#ML_data$V1 <- gene_names$name
ML_data <- ML_data %>%
tibble() %>%
column_to_rownames(var = "V1") %>%  # Set first column as row names
t() %>%  # Transpose the data for easier handling
as_tibble(rownames = "sample")  # Convert to tibble and keep row names as a column
# Load validation data
ML_validation <- fread("../../ALLDEGs/cpm_table_log_30.csv")
# Convert Ensembl IDs to gene names
gene_names <- gconvert(query = ML_validation$V1, organism = "hsapiens",
target = "ENSG", mthreshold = Inf, filter_na = TRUE)
ML_validation$V1 <- gene_names$name
ML_validation <- ML_validation %>%
tibble() %>%
distinct(V1, .keep_all = TRUE) %>%
column_to_rownames(var = "V1") %>%  # Set first column as row names
t() %>%  # Transpose the data for easier handling
as_tibble(rownames = "sample")  # Convert to tibble and keep row names as a column
# Load metadata for sample subtypes
# Read the metadata file and rename the sample column for consistency
samples_70_explained <- read_csv("../../ALLDEGs/info_samples_70.csv") %>%
tibble() %>%
select(-...1)
samples_30_explained <- read_csv("../../ALLDEGs/info_samples_30.csv") |>
tibble() |>
select(-...1)
# Merge and filter the data 70
ML_data <- ML_data %>%
left_join(samples_70_explained, by = "sample") %>%  # Join tumor/control info
filter(condition != "H") %>%  # Filter out samples with condition 'H'
select(-c(type, age, replicate, condition))  # Remove unnecessary columns
# Convert to data frame and set sample as row names
ML_data <- data.frame(ML_data)  # Convert tibble to data frame
rownames(ML_data) <- ML_data$sample  # Set row names to 'sample'
ML_data$sample <- NULL  # Remove the sample column
# Merge and filter the data 30
ML_validation <- ML_validation %>%
left_join(samples_30_explained, by = "sample") %>%  # Join tumor/control info
filter(condition != "H") %>%  # Filter out samples with condition 'H'
select(-c(type, age, replicate, condition))  # Remove unnecessary columns
# Convert to data frame and set sample as row names
ML_validation <- data.frame(ML_validation)  # Convert tibble to data frame
rownames(ML_validation) <- ML_validation$sample  # Set row names to 'sample'
ML_validation$sample <- NULL  # Remove the sample column
# Identify indices of samples with 'Unknown' cell types
unknown_indices <- which(ML_data$Cell_type == 'Unknown')  # Find 'Unknown' cell type indices
table(ML_data$Cell_type)  # Display counts of each cell type
#fake_data <- read_csv("input/fake_data_FM.csv") |>  filter(Cell_type != "PreB")
#colnames(fake_data) <- colnames(ML_data)
#ML_data <- rbind(ML_data, fake_data)
# Clean up unnecessary variables to free memory
#rm(info_samples_Tumor_Control, samples_subtypes_explained)  # Remove temporary metadata
# Number encoding of categorical values
# Specify the columns to be label encoded
columns_to_encode <- c("Cell_type")
# Create separate datasets based on 'Cell_type'
# Create a new data frame without rows where 'Cell_type' is "Unknown"
my_data_train <- subset(ML_data, Cell_type != "Unknown")
# Create a separate dataset for rows where 'Cell_type' is "Unknown"
Unknown_data <- subset(ML_data, Cell_type == "Unknown")
# Convert specified columns to factor type
my_data_train <- my_data_train %>%
mutate_at(columns_to_encode, as.factor)  # Convert 'type' to factor
Unknown_data <- Unknown_data %>%
mutate_at(columns_to_encode, as.factor)  # Convert 'type' to factor in Unknown_data
# Get the levels from ML_data
common_levels <- levels(my_data_train$Cell_type)
# Set the levels
ML_validation$Cell_type <- factor(ML_validation$Cell_type, levels = common_levels)
## Optional: Convert factors to numeric (commented out)
# train_data_numeric <- my_data_train %>% mutate_at(columns_to_encode, as.numeric)  # Convert factors to numeric
# Create a dictionary-like structure to store the levels of categorical variables
## The order corresponds to the number (commented out)
# my_levels <- list(Cell_type = levels(my_data_train$Cell_type))  # Store levels of 'Cell_type'
# Clean up unnecessary variables to free memory
rm(columns_to_encode)
set.seed(RANDOM_SEED)
# === Train/Test Split ===
train_data <- my_data_train
test_data  <- ML_validation
cli::cli_h3("\n[INFO] Train ML data before NCL:\n")
print(table(train_data$Cell_type))
Classes <- list("B" = 8, "PreB" = 1, "PreT" = 4, "T" = 2)
# === Advanced Class Balancing using UBL ===
train_data <- SMOGNClassif(Cell_type ~ ., train_data, C.perc =  Classes,k = 5)
#train_data <- SmoteClassif(Cell_type ~ ., train_data, C.perc = list("B" = 8, "PreB" = 1, "PreT" = 4, "T" = 2),k = 5)
cli::cli_h3("\n[INFO] Train ML data after SMOTEGNn:\n")
print(table(train_data$Cell_type))
train_data <- NCLClassif(Cell_type ~ ., train_data,k = 5) #,Cl = c("B", "PreT", "T"),
cli::cli_h3("\n[INFO] Train ML data after NCL:\n")
print(table(train_data$Cell_type))
#train_data <- train_data |> mutate(Weights = hardhat::importance_weights(as.integer(Cell_type)))
# Calculate class frequencies
class_counts <- table(train_data$Cell_type)
total_samples <- sum(class_counts)
num_classes <- length(class_counts)
# Compute weights inversely proportional to class frequencies
class_weights <- (total_samples / (num_classes * class_counts))^0.5
class_weights <- c(as.numeric(class_weights))
class_weights <- class_weights / mean(class_weights)
names(class_weights) <- names(class_counts)
print(round(class_weights, 3))
# Assign weights to each observation
train_data <- train_data %>%
mutate(Weights = class_weights[as.character(Cell_type)])
train_data$Weights <- hardhat::new_importance_weights(c(train_data$Weights))
# === Preprocessing Recipe ===
recipe <- recipe(Cell_type ~ ., data = train_data)
# === Recipe Prep and Bake ===
rec_prep   <- prep(recipe, training = train_data)
baked_data <- bake(rec_prep, new_data = NULL)
cli::cli_h3("\n[INFO] Baked (final) ML training data:\n")
print(table(baked_data$Cell_type))
# === Cross-validation Splits ===
v_folds <- vfold_cv(train_data, strata = Cell_type, v = 5, repeats = 3)
#nested_cv(data = train_data)
# === Evaluation Metrics ===
class_metrics <- metric_set(kap, mcc, bal_accuracy, f_meas, roc_auc, ppv)
# Define the vector of package names needed for the project
PACKAGES <- c(
"xgboost",         # Extreme Gradient Boosting algorithm for classification/regression
"ggplot2",         # Data visualization package based on the grammar of graphics
"RColorBrewer",    # Color palettes for enhanced data visualization
"tidymodels",      # Tidy modeling framework for machine learning workflows
"tidyverse",       # Collection of packages for data manipulation, analysis, and visualization
"finetune",        # Tools for model tuning and optimization
"themis",          # Techniques for handling class imbalance in machine learning
"gprofiler2",      # Gene enrichment and pathway analysis for gene lists
"future",          # Enables parallel processing for faster computations
"data.table",      # High-performance data manipulation package
"gt",              # For creating display tables
#"gtsummary",       # For summary tables with statistics and formatted output
"butcher",         # Reduces memory load of models by removing unnecessary objects
"UBL",              # Utility-based learning techniques for imbalanced data
"ranger",
"kernelshap",
"shapviz"
)
# Use purrr::walk to load all packages
purrr::walk(PACKAGES, library, character.only = TRUE)
gc()
plan(multisession, workers = 10)
nbrOfWorkers()
RANDOM_SEED <- 1234
#plan("multisession", workers = 15,)
#plan(multisession, workers = 15)
#plan(sequential)
